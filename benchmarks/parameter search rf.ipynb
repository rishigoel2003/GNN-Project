{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rishi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "18 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rishi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Rishi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 340, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Rishi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Rishi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_samples' parameter of RandomForestRegressor must be None, a float in the range (0.0, 1.0] or an int in the range [1, inf). Got 1.0000000000000004 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Rishi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [-0.97225657 -1.29737419 -1.23133239 -1.13601329 -1.11073346 -1.24988581\n",
      " -0.92969849 -1.18135225 -0.97466883 -1.02232636         nan -1.0282148\n",
      " -1.27673207 -1.0063577  -1.02566019 -1.10780878 -1.00179345 -1.28866603\n",
      " -1.06619569 -0.94639162 -1.02041396 -0.9670397  -1.24451562 -1.00094431\n",
      " -1.03000404 -1.11542858 -0.94185505 -1.02345636 -0.95168643 -1.03428216\n",
      " -0.94368455 -0.93185438 -0.95478526 -1.13424227 -1.0165844  -0.97527052\n",
      " -1.10692122 -0.9397325  -0.93506497 -1.00855504 -1.2651472  -1.11525347\n",
      " -0.96396764 -1.10130217 -1.04433064 -1.30505267 -0.92502878 -1.05304849\n",
      " -0.9502875  -1.04483911 -0.93659312 -0.9466166  -1.25456797 -0.99004282\n",
      " -1.11977908 -0.94075472 -1.22980624 -1.16117529 -1.18733531 -0.99794654\n",
      " -0.98843482 -1.03257539 -1.02877526 -1.05335537 -0.97856935 -1.01218415\n",
      " -1.23671222         nan -1.09736551 -1.0623552  -1.01586737 -0.98220368\n",
      " -1.09934571 -0.9841423  -1.05619048 -1.08096807         nan -0.93516125\n",
      " -0.94813809 -1.13499904 -0.98459851 -0.95157181         nan -0.95746475\n",
      " -1.04252187 -1.05833066 -1.15322304 -0.98122574 -1.04520153 -1.08490247\n",
      " -0.98174761 -0.94315537 -0.95902626 -1.06665692         nan         nan\n",
      " -1.18349861 -1.27840687 -1.0029708  -0.97504271]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(max_depth=9, max_features=0.8999999999999999,\n",
      "                      max_samples=0.5500000000000002, n_estimators=157)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import  BottomUp, TopDown, MinTrace\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "    \n",
    "\n",
    "callbacks = [lgb.log_evaluation(period=0)]\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "sales_train_eval = pd.read_csv('sales_train_evaluation.csv')\n",
    "sell_price = pd.read_csv('sell_prices.csv')\n",
    "calendar = pd.read_csv('calendar.csv')\n",
    "\n",
    "foods = pd.read_csv('List_of_foods.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#making the summing matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# rows / columns\n",
    "list1 = ['Total', 'CA','CA_1','CA_2','CA_3','CA_4','TX','TX_1','TX_2','TX_3','WI','WI_1','WI_2','WI_3']\n",
    "list2 = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']\n",
    "S = np.zeros((len(list1), len(list2)))\n",
    "\n",
    "S = pd.DataFrame(S); S.index = list1; S.columns = list2\n",
    "\n",
    "\n",
    "# encode the hierarchical structure\n",
    "S.loc['Total'] = 1\n",
    "S.loc['CA'][['CA_1','CA_2','CA_3', 'CA_4']] = 1\n",
    "S.loc['TX'][['TX_1','TX_2','TX_3']] = 1\n",
    "S.loc['WI'][['WI_1','WI_2','WI_3']] = 1\n",
    "for x in S.columns:\n",
    "    S.loc[x][x]= 1\n",
    "S = S.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tags = {}\n",
    "tags['Country'] = np.array(['Total'], dtype=object)\n",
    "tags['Country/State'] = np.array(['CA', 'TX', 'WI'], dtype=object)\n",
    "tags['Country/State/Store'] = np.array(['CA_1', 'CA_2', 'CA_3', 'CA_4',  \n",
    "                                        'TX_1', 'TX_2', 'TX_3',\n",
    "                                        'WI_1', 'WI_2', 'WI_3'], dtype=object)\n",
    "\n",
    "\n",
    "\n",
    "horizon = 28\n",
    "\n",
    "\n",
    "\n",
    "def label_encoding(train, feature):\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(train[feature].values.astype(str))\n",
    "    train[feature] = encoder.fit_transform(train[feature].values.astype(str))\n",
    "    \n",
    "    return train[feature]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "col = ['event_name_1', 'event_type_1',\n",
    "       'event_name_2', 'event_type_2', 'wday','month', 'year','snap_CA','snap_TX','snap_WI', 'value_lag_1', 'value_lag_2', \n",
    "       'value_lag_3', 'value_lag_6', 'value_lag_12', 'value_lag_24', 'value_lag_36', 'rolling_value_mean']\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'metric': ['rmse'],\n",
    "    'objective': ['regression'],\n",
    "    'n_jobs': [-1],\n",
    "    #'seed': [236],\n",
    "    'learning_rate': [0.28],\n",
    "    'bagging_fraction': [0.75],\n",
    "    'bagging_freq': [5],\n",
    "    'colsample_bytree': [0.75],\n",
    "    'force_row_wise' : [True],\n",
    "    'verbose':[-1],\n",
    "    'num_leaves':[31]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "food_num=1\n",
    "\n",
    "\n",
    "product_id = foods.loc[food_num].at[\"Foods\"]\n",
    "\n",
    "\n",
    "product_data = sales_train_eval[sales_train_eval['item_id'].str.contains(product_id)]\n",
    "product_sell_price = sell_price[sell_price['item_id'].str.contains(product_id)]\n",
    "\n",
    "\n",
    "df = pd.melt(\n",
    "product_data,\n",
    "id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "var_name='d',\n",
    "value_name='value').dropna()\n",
    "df = pd.merge(df, calendar, on='d', how='left')\n",
    "\n",
    "df = df[(df['date'] > '2014-01-01')]\n",
    "\n",
    "df[\"event_name_1\"] = df[\"event_name_1\"].fillna(\"no_event\")\n",
    "df[\"event_name_2\"] = df[\"event_name_2\"].fillna(\"no_event\")\n",
    "df[\"event_type_1\"] = df[\"event_type_1\"].fillna(\"no_event\")\n",
    "df[\"event_type_2\"] = df[\"event_type_2\"].fillna(\"no_event\")\n",
    "\n",
    "\n",
    "df_stores = df.groupby(['date', 'store_id','wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI'])[['value']].sum()\n",
    "df_stores.reset_index(inplace=True)\n",
    "df_stores = df_stores.T.reset_index(drop=True).T\n",
    "df_stores.columns = ['d', 'unique_id', 'wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI','sales']\n",
    "\n",
    "\n",
    "df_state = df.groupby(['date', 'state_id','wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI'])[['value']].sum()\n",
    "df_state.reset_index(inplace=True)\n",
    "df_state = df_state.T.reset_index(drop=True).T\n",
    "df_state.columns = ['d', 'unique_id','wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI', 'sales']\n",
    "\n",
    "\n",
    "\n",
    "df_total = df.groupby(['date','wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI'])[['value']].sum()\n",
    "df_total.reset_index(inplace=True)\n",
    "df_total['unique_id'] = 'Total'\n",
    "df_total.columns = ['d','wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI','sales', 'unique_id']\n",
    "\n",
    "df_all = pd.concat([df_stores, df_state, df_total], axis = 0)\n",
    "\n",
    "df_all.columns = ['ds','unique_id','wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI', 'y']\n",
    "df_all['ds'] = pd.to_datetime(df_all['ds'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Introduce lags\n",
    "lags = [1,2,3,6,12,24,36]\n",
    "for lag in lags:\n",
    "    df_all['value_lag_'+str(lag)] = df_all.groupby(['unique_id'],as_index=False)['y'].shift(lag)\n",
    "\n",
    "for lag in lags:\n",
    "    df_all['value_lag_'+str(lag)] = df_all['value_lag_'+str(lag)].fillna(0)\n",
    "\n",
    "df_all['rolling_value_mean'] = df_all.groupby(['unique_id'])['y'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16)\n",
    "df_all['rolling_value_mean'] = df_all['rolling_value_mean'].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_all['wday']  = label_encoding(df_all,\"wday\" )\n",
    "df_all['month']  = label_encoding(df_all,\"month\" )\n",
    "df_all['year']  = label_encoding(df_all,\"year\" )\n",
    "df_all['snap_CA']  = label_encoding(df_all,\"snap_CA\" )\n",
    "df_all['snap_TX']  = label_encoding(df_all,\"snap_TX\" )\n",
    "df_all['snap_WI']  = label_encoding(df_all,\"snap_WI\" )\n",
    "\n",
    "\n",
    "df_all['event_name_1']  = label_encoding(df_all,\"event_name_1\" )\n",
    "df_all['event_name_2']  = label_encoding(df_all,\"event_name_2\" )\n",
    "df_all['event_type_1']  = label_encoding(df_all,\"event_type_1\" )\n",
    "df_all['event_type_2']  = label_encoding(df_all,\"event_type_2\" )\n",
    "\n",
    "\n",
    "\n",
    "x_test = df_all.groupby('unique_id').tail(horizon)\n",
    "x_train = df_all.drop(x_test.index)\n",
    "x_val = x_train.groupby('unique_id').tail(horizon)\n",
    "x_train = x_train.drop(x_val.index)\n",
    "\n",
    "\n",
    "x_train['y'] = x_train['y'].astype(float)\n",
    "x_test['y'] = x_test['y'].astype(float)\n",
    "x_val['y'] = x_val['y'].astype(float)\n",
    "\n",
    "\n",
    "y_train = x_train['y']\n",
    "y_test = x_test['y']\n",
    "y_val = x_val['y']\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Step 1: Define parameter distribution for randomized search\n",
    "param_distribution = {\n",
    "    'max_depth': np.arange(1, 10),\n",
    "    'max_samples':np.arange(0.1,1.01,0.05),\n",
    "    'n_estimators': np.arange(100, 200),\n",
    "    'max_features':np.arange(0.2,1.001,0.05),\n",
    "    \n",
    "}\n",
    "\n",
    "# Step 2: Initialize LGBMRegressor estimator\n",
    "estimator = RandomForestRegressor()\n",
    "\n",
    "# Step 3: Initialize Randomized Search with 3-fold cross validation and fit the model\n",
    "model = RandomizedSearchCV(estimator=estimator,\n",
    "                           param_distributions=param_distribution,\n",
    "                           n_iter=100,  # Number of random combinations to try\n",
    "                           cv=3,\n",
    "                           n_jobs=4,\n",
    "                           scoring='neg_root_mean_squared_error')\n",
    "model.fit(x_train[col], y_train)\n",
    "\n",
    "# Step 4: Print best parameters\n",
    "best_params = model.best_estimator_\n",
    "print(best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
