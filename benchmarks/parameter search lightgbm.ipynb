{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "LGBMRegressor(colsample_bytree=0.71, lambda_l1=1.0,\n",
      "              learning_rate=0.11000000000000001, max_depth=4,\n",
      "              min_child_samples=3, n_estimators=272, num_leaves=20,\n",
      "              objective='regression')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import  BottomUp, TopDown, MinTrace\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "    \n",
    "\n",
    "callbacks = [lgb.log_evaluation(period=0)]\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "sales_train_eval = pd.read_csv('sales_train_evaluation.csv')\n",
    "sell_price = pd.read_csv('sell_prices.csv')\n",
    "calendar = pd.read_csv('calendar.csv')\n",
    "\n",
    "foods = pd.read_csv('List_of_foods.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#making the summing matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# rows / columns\n",
    "list1 = ['Total', 'CA','CA_1','CA_2','CA_3','CA_4','TX','TX_1','TX_2','TX_3','WI','WI_1','WI_2','WI_3']\n",
    "list2 = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']\n",
    "S = np.zeros((len(list1), len(list2)))\n",
    "\n",
    "S = pd.DataFrame(S); S.index = list1; S.columns = list2\n",
    "\n",
    "\n",
    "# encode the hierarchical structure\n",
    "S.loc['Total'] = 1\n",
    "S.loc['CA'][['CA_1','CA_2','CA_3', 'CA_4']] = 1\n",
    "S.loc['TX'][['TX_1','TX_2','TX_3']] = 1\n",
    "S.loc['WI'][['WI_1','WI_2','WI_3']] = 1\n",
    "for x in S.columns:\n",
    "    S.loc[x][x]= 1\n",
    "S = S.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tags = {}\n",
    "tags['Country'] = np.array(['Total'], dtype=object)\n",
    "tags['Country/State'] = np.array(['CA', 'TX', 'WI'], dtype=object)\n",
    "tags['Country/State/Store'] = np.array(['CA_1', 'CA_2', 'CA_3', 'CA_4',  \n",
    "                                        'TX_1', 'TX_2', 'TX_3',\n",
    "                                        'WI_1', 'WI_2', 'WI_3'], dtype=object)\n",
    "\n",
    "\n",
    "\n",
    "horizon = 28\n",
    "\n",
    "\n",
    "\n",
    "def label_encoding(train, feature):\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(train[feature].values.astype(str))\n",
    "    train[feature] = encoder.fit_transform(train[feature].values.astype(str))\n",
    "    \n",
    "    return train[feature]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "col = ['event_name_1', 'event_type_1',\n",
    "       'event_name_2', 'event_type_2', 'wday','month', 'year','snap_CA','snap_TX','snap_WI', 'value_lag_1', 'value_lag_2', \n",
    "       'value_lag_3', 'value_lag_6', 'value_lag_12', 'value_lag_24', 'value_lag_36', 'rolling_value_mean']\n",
    "\n",
    "\n",
    "# hyperparameters = {\n",
    "#     'boosting_type': ['gbdt'],\n",
    "#     'metric': ['rmse'],\n",
    "#     'objective': ['regression'],\n",
    "#     'n_jobs': [-1],\n",
    "#     #'seed': [236],\n",
    "#     'learning_rate': [0.28],\n",
    "#     'bagging_fraction': [0.75],\n",
    "#     'bagging_freq': [5],\n",
    "#     'colsample_bytree': [0.75],\n",
    "#     'force_row_wise' : [True],\n",
    "#     'verbose':[-1],\n",
    "#     'num_leaves':[31]\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "food_num=5\n",
    "\n",
    "\n",
    "product_id = foods.loc[food_num].at[\"Foods\"]\n",
    "\n",
    "\n",
    "product_data = sales_train_eval[sales_train_eval['item_id'].str.contains(product_id)]\n",
    "product_sell_price = sell_price[sell_price['item_id'].str.contains(product_id)]\n",
    "\n",
    "\n",
    "df = pd.melt(\n",
    "product_data,\n",
    "id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "var_name='d',\n",
    "value_name='value').dropna()\n",
    "df = pd.merge(df, calendar, on='d', how='left')\n",
    "\n",
    "df = df[(df['date'] > '2014-01-01')]\n",
    "\n",
    "df[\"event_name_1\"] = df[\"event_name_1\"].fillna(\"no_event\")\n",
    "df[\"event_name_2\"] = df[\"event_name_2\"].fillna(\"no_event\")\n",
    "df[\"event_type_1\"] = df[\"event_type_1\"].fillna(\"no_event\")\n",
    "df[\"event_type_2\"] = df[\"event_type_2\"].fillna(\"no_event\")\n",
    "\n",
    "\n",
    "df = df[(df['date'] > '2015-01-01')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_stores = df.groupby(['date', 'store_id','wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI'])[['value']].sum()\n",
    "df_stores.reset_index(inplace=True)\n",
    "df_stores = df_stores.T.reset_index(drop=True).T\n",
    "df_stores.columns = ['d', 'unique_id', 'wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI','sales']\n",
    "\n",
    "\n",
    "df_state = df.groupby(['date', 'state_id','wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI'])[['value']].sum()\n",
    "df_state.reset_index(inplace=True)\n",
    "df_state = df_state.T.reset_index(drop=True).T\n",
    "df_state.columns = ['d', 'unique_id','wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI', 'sales']\n",
    "\n",
    "\n",
    "\n",
    "df_total = df.groupby(['date','wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI'])[['value']].sum()\n",
    "df_total.reset_index(inplace=True)\n",
    "df_total['unique_id'] = 'Total'\n",
    "df_total.columns = ['d','wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI','sales', 'unique_id']\n",
    "\n",
    "df_all = pd.concat([df_stores, df_state, df_total], axis = 0)\n",
    "\n",
    "df_all.columns = ['ds','unique_id','wday','month','year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI', 'y']\n",
    "df_all['ds'] = pd.to_datetime(df_all['ds'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Introduce lags\n",
    "lags = [1,2,3,6,12,24,36]\n",
    "for lag in lags:\n",
    "    df_all['value_lag_'+str(lag)] = df_all.groupby(['unique_id'],as_index=False)['y'].shift(lag)\n",
    "\n",
    "for lag in lags:\n",
    "    df_all['value_lag_'+str(lag)] = df_all['value_lag_'+str(lag)].fillna(0)\n",
    "\n",
    "df_all['rolling_value_mean'] = df_all.groupby(['unique_id'])['y'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16)\n",
    "df_all['rolling_value_mean'] = df_all['rolling_value_mean'].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_all['wday']  = label_encoding(df_all,\"wday\" )\n",
    "df_all['month']  = label_encoding(df_all,\"month\" )\n",
    "df_all['year']  = label_encoding(df_all,\"year\" )\n",
    "df_all['snap_CA']  = label_encoding(df_all,\"snap_CA\" )\n",
    "df_all['snap_TX']  = label_encoding(df_all,\"snap_TX\" )\n",
    "df_all['snap_WI']  = label_encoding(df_all,\"snap_WI\" )\n",
    "\n",
    "\n",
    "df_all['event_name_1']  = label_encoding(df_all,\"event_name_1\" )\n",
    "df_all['event_name_2']  = label_encoding(df_all,\"event_name_2\" )\n",
    "df_all['event_type_1']  = label_encoding(df_all,\"event_type_1\" )\n",
    "df_all['event_type_2']  = label_encoding(df_all,\"event_type_2\" )\n",
    "\n",
    "\n",
    "\n",
    "x_test = df_all.groupby('unique_id').tail(horizon)\n",
    "x_train = df_all.drop(x_test.index)\n",
    "x_val = x_train.groupby('unique_id').tail(horizon)\n",
    "x_train = x_train.drop(x_val.index)\n",
    "\n",
    "\n",
    "x_train['y'] = x_train['y'].astype(float)\n",
    "x_test['y'] = x_test['y'].astype(float)\n",
    "x_val['y'] = x_val['y'].astype(float)\n",
    "\n",
    "\n",
    "y_train = x_train['y']\n",
    "y_test = x_test['y']\n",
    "y_val = x_val['y']\n",
    "\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, max_error, explained_variance_score, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# # Step 1: Define parameter grid for hyperparameter tuning\n",
    "# param_grid = {\n",
    "    \n",
    "#     'n_estimators': [270,300],\n",
    "#     'max_depth': [3,4,5],\n",
    "#     'subsample': [0.7,0.6,0.8],\n",
    "#     'min_child_samples': [1,2,3],\n",
    "#     'bagging_fraction': [0.9,0.95],\n",
    "#     'bagging_freq': [2,3,4],\n",
    "#     'colsample_bytree': [0.74, 0.75],\n",
    "#     'num_leaves':[10,19,25],  \n",
    "#     'learning_rate': [0.13,0.15]\n",
    "# }\n",
    "\n",
    "# # Step 2: Initialize LGBMRegressor estimattor\n",
    "# estimator = lgb.LGBMRegressor(objective='regression')\n",
    "\n",
    "# # Step 3: Initalise Grid Search with 3-fold cross validation and fit model\n",
    "# model = GridSearchCV(estimator=estimator, \n",
    "#                      param_grid=param_grid,\n",
    "#                      cv=3, \n",
    "#                      n_jobs=-1, \n",
    "#                      scoring='neg_root_mean_squared_error',\n",
    "#                      verbose = 2)\n",
    "# with tqdm(total=len(param_grid)) as pbar:\n",
    "#    model.fit(x_train[col], y_train, callback=lambda *_: pbar.update())\n",
    "\n",
    "# # Step 4: Print best parameters\n",
    "# best_params = model.best_estimator_\n",
    "# print(best_params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Step 1: Define parameter distribution for randomized search\n",
    "param_distribution = {\n",
    "    'n_estimators': np.arange(270, 275),\n",
    "    'max_depth': np.arange(4, 8),\n",
    "    'min_child_samples': np.arange(1, 4),\n",
    "    'colsample_bytree': np.arange(0.70, 0.82, 0.01),\n",
    "    'num_leaves': np.arange(10, 25),\n",
    "    'learning_rate': np.arange(0.10, 0.15, 0.005),\n",
    "    'lambda_l1':np.arange(0, 1.01, 0.05),\n",
    "    #'lambda_l2':np.arange(0, 1.01, 0.05)\n",
    "}\n",
    "\n",
    "# Step 2: Initialize LGBMRegressor estimator\n",
    "estimator = lgb.LGBMRegressor(objective='regression')\n",
    "\n",
    "# Step 3: Initialize Randomized Search with 3-fold cross validation and fit the model\n",
    "model = RandomizedSearchCV(estimator=estimator,\n",
    "                           param_distributions=param_distribution,\n",
    "                           n_iter=2000,  # Number of random combinations to try\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           scoring='neg_root_mean_squared_error')\n",
    "model.fit(x_train[col], y_train)\n",
    "\n",
    "# Step 4: Print best parameters\n",
    "best_params = model.best_estimator_\n",
    "print(best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
